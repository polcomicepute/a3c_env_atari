{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ffa9690",
   "metadata": {},
   "source": [
    "# Training Atari Game Agents Using A3C  \n",
    "## Run\n",
    "python3 main.py --env-name \"BoxingNoFrameskip-v4\" --num-processes 12 --exp eval\n",
    "python3 main.py --env-name \"BoxingNoFrameskip-v4\" --num-processes 12 --exp train\n",
    "\n",
    "## 실험 환경\n",
    "- Apple M1 Pro 칩\n",
    "\n",
    "  8코어 CPU(성능 코어 6개 및 효율 코어 2개)\n",
    "- 12 multi-processes\n",
    "\n",
    "## 실험 조건\n",
    "- 5000,000 step만 실행 \n",
    "- 실행한 Atari Game 종류\n",
    "    1. Pong\n",
    "        ![Alt text](./img/pong.gif)\n",
    "\n",
    "    2. Breakout\n",
    "        ![Alt text](./img/breakout.gif)\n",
    "        \n",
    "    3. SpaceInvaders\n",
    "        ![Alt text](./img/space_invaders.gif)\n",
    "        \n",
    "    4. Boxing\n",
    "        ![Alt text](./img/boxing.gif)\n",
    "        \n",
    "    5. IceHockey\n",
    "        ![Alt text](./img/ice_hockey.gif)\n",
    "- WandB 이용\n",
    "\n",
    "## Hyperparameters \n",
    "- Learning rate = 0.0001\n",
    "- No Learning Scheduler (성적이 좋지 않음)\n",
    "- Adam Optimizer \n",
    "- Seed = 1 + rank\n",
    "- Image Size (H, W, C): [210, 160, 3] => [84, 84, 1] \n",
    "- N-step = 20\n",
    "- Gamma = 0.99\n",
    "- Entropy Coefficient = 0.01\n",
    "- Value Loss Coefficient = 0.5\n",
    "\n",
    "\n",
    "\n",
    "## Input Representation & Env Customization\n",
    "Atari Wrapper 이용하여 Environment Customization\n",
    "- __NoopResetEnv(noop_max=30)__\n",
    "\n",
    "    Reset후 no-ops에 해당하는 숫자를 랜덤하게 취해서 initial state sampling  \n",
    "    \n",
    "        \n",
    "- __MaxAndSkipEnv(skip=4)__  \n",
    "\n",
    "    Return only every skip-th frame (frameskipping)\n",
    "    Step the environment with the given action Repeat action, sum reward, and max over last observations.\n",
    "    frame skipping    \n",
    "    \n",
    "\n",
    "- __EpisodicLifeEnv__  \n",
    "\n",
    "    Make end-of-life == end-of-episode, but only reset on true game over. Done by DeepMind for the DQN and co. since it helps value estimation.(Agent가 죽는 시점을 episode끝으로 보고, done을 반환 후 reset한다.)  \n",
    "    \n",
    "\n",
    "- __ClipRewardEnv__  \n",
    "\n",
    "    Clips the reward to {+1, 0, -1} by its sign.  \n",
    "    \n",
    "\n",
    "\n",
    "- __WarpFrame (Input Representation)__\n",
    "\n",
    "    Convert to grayscale and warp frames to 84x84 (default) as done in the Nature paper and later work.\n",
    "\n",
    "\n",
    "https://stable-baselines3.readthedocs.io/en/master/common/atari_wrappers.html  \n",
    "\n",
    "https://towardsdatascience.com/deep-q-network-dqn-i-bce08bdf2af\n",
    "\n",
    "<img src=\"./img/representation.png\" width=\"60%\" height=\"60%\" >\n",
    "\n",
    "\n",
    "  \n",
    "    \n",
    "## Model Archiecture\n",
    "\n",
    "<img src=\"./img/model.png\" width=\"110%\" height=\"110%\" >\n",
    "\n",
    "  \n",
    "## Loss\n",
    "- __Advantage__ \n",
    "- __N-step Return__\n",
    "- __Entropy Regularization Term 이용__\n",
    "\n",
    "### 최종 Loss  \n",
    "### $L = L_{\\pi}+c_vL_v+c_eL_{entropy}$    \n",
    "- $L$ = 최종 Loss\n",
    "- $L_{\\pi}$ = Actor Loss\n",
    "- $L_{v}$ = Critic Loss\n",
    "- $L_{entropy}$ = Entropy Regularization Term\n",
    "- $c_v$ = value\\ coefficient\n",
    "- $c_e$ = entropy\\ coefficient  \n",
    "\n",
    "### Critic Loss\n",
    "### $L_v = (r_{t+1}+ \\gamma r_{t+2}+ \\gamma^2 r_{t+2}... + \\gamma^{19} V_v(s_{t+20})-V_v(s_t))^2 +(r_{t+2}+ \\gamma r_{t+3}+ \\gamma^2 r_{t+4}... + \\gamma^{18} V_v(s_{t+20})-V_v(s_{t+1}))^2 + ... + (r_{t+19}+ \\gamma V_v(s_{t+20})-V_v(s_{t+19}))^2$\n",
    "\n",
    "### Actor Loss\n",
    "<img src=\"./img/n-step.png\" width=\"70%\" height=\"70%\" >\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "## Experiment Score Graph\n",
    "### Pong\n",
    "- Train\n",
    "<img src=\"./img/pong_train.png\" width=\"120%\" height=\"120%\" >\n",
    "\n",
    "\n",
    "\n",
    "- Test\n",
    "<img src=\"./img/pong_eval.png\" width=\"100%\" height=\"100%\" >\n",
    "\n",
    "### SpaceInvaders\n",
    "- Train\n",
    "<img src=\"./img/spaceinvaders_train.png\" width=\"120%\" height=\"120%\" >\n",
    "\n",
    "\n",
    "\n",
    "- Test\n",
    "<img src=\"./img/spaceinvaders_eval.png\" width=\"100%\" height=\"100%\" >\n",
    "\n",
    "### Breakout\n",
    "- Train\n",
    "<img src=\"./img/breakout_train.png\" width=\"120%\" height=\"120%\" >\n",
    "\n",
    "\n",
    "\n",
    "- Test\n",
    "<img src=\"./img/breakout_eval.png\" width=\"100%\" height=\"100%\" >\n",
    "\n",
    "### IceHockey\n",
    "- Train\n",
    "<img src=\"./img/icehockey_train.png\" width=\"120%\" height=\"120%\" >\n",
    "\n",
    "\n",
    "- Test\n",
    "<img src=\"./img/icehockey_eval.png\" width=\"100%\" height=\"100%\" >\n",
    "\n",
    "\n",
    "### Boxing\n",
    "- Train\n",
    "<img src=\"./img/boxing_train.png\" width=\"120%\" height=\"120%\" >\n",
    "\n",
    "\n",
    "- Test\n",
    "<img src=\"./img/boxing_eval.png\" width=\"100%\" height=\"100%\" >  \n",
    "\n",
    "\n",
    "## Shared Model 작동 원리\n",
    "- Multiprocessing of Pytorch\n",
    "다른 process들에게 shared views on the same data를 제공하는 shared_memory를 이용한다. \n",
    "tensor/storage가 shared_memory로 이동하면 copy필요없이 텐서나 스토리지를 자유롭게 다른 process로 전송할 수 있다.\n",
    "<img src=\"./img/shared_memory.png\" width=\"70%\" height=\"70%\" >    \n",
    "Global weight를 지니는 shared model은 \n",
    "<img src=\"./img/codeofsharedmemory.png\" width=\"60%\" height=\"60%\" >     \n",
    "\n",
    "이와같이 선언하는데, shared memory를 통해서 global memory를 이용하겠다는 의미다.  \n",
    "하지만 참고한 코드중에, shared_grad에 local_grad를 처음 프로세스가 생성되고 한번만 할당하는 것을 발견했다.   \n",
    "<img src=\"./img/grad.png\" width=\"60%\" height=\"60%\" >    \n",
    "\n",
    "맞는 코드인가 싶어서 찾아보니, multiprocessing에서 각 프로세스는 shared memory의 data(weight)는 공유하지만 grad는 공유하지 않는다고 한다. grad는 각 프로세스마다 local로 가지며, 각자local model의 loss backward를 통해서 grad를 계산하고 local grad를 통해 global model의 parameter를 올린 optimizer를 이용해서 global weight를 갱신한다.  \n",
    "__한마디로 각 프로세스마다 각자의 grad를 가지고 배울 수 있도록 하기 위해서라고 할 수 있다.__\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "- 참고한 git \n",
    "https://github.com/ikostrikov/pytorch-a3c\n",
    "\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1669cd1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8926c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
